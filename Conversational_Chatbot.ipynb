{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOitCsQIoKEndIAZ5F4kmD7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utkarshg1/Test1/blob/main/Conversational_Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9PLGVO-FdCq",
        "outputId": "61a91293-b967-4179-f8d8-d0d2cef55f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.49.1)\n",
            "Collecting groq\n",
            "  Downloading groq-0.33.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.119.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.3)\n",
            "Requirement already satisfied: gradio-client==1.13.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.3)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.10)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.1)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.3->gradio) (15.0.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading groq-0.33.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.33.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gradio groq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "api_key = userdata.get(\"GROQ_API_KEY\")"
      ],
      "metadata": {
        "id": "E_EgstQXHsxJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to access the llm"
      ],
      "metadata": {
        "id": "mqC0TVhnJ3HV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from groq import Groq\n",
        "\n",
        "client = Groq(api_key = api_key)"
      ],
      "metadata": {
        "id": "5N3Ev5_TJAPh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_completion = client.chat.completions.create(\n",
        "    messages = [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Write python code to generate prime numbers\"\n",
        "    }],\n",
        "    model = \"llama-3.3-70b-versatile\",\n",
        "    temperature = 0.7,\n",
        "    max_tokens = 2048\n",
        ")\n",
        "\n",
        "print(chat_completion.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmR1xWzcKEL_",
        "outputId": "3cc046e5-c58a-46ca-c750-1ac8a4b3bb2d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**Prime Number Generation in Python**\n",
            "======================================\n",
            "\n",
            "The following code generates prime numbers using two different methods: a simple trial division method and the Sieve of Eratosthenes algorithm.\n",
            "\n",
            "### Method 1: Trial Division\n",
            "\n",
            "This method checks each number up to a given limit to see if it is prime.\n",
            "\n",
            "```python\n",
            "def generate_primes_trial_division(n):\n",
            "    \"\"\"\n",
            "    Generate prime numbers up to n using trial division.\n",
            "\n",
            "    Args:\n",
            "        n (int): The upper limit.\n",
            "\n",
            "    Returns:\n",
            "        list: A list of prime numbers up to n.\n",
            "    \"\"\"\n",
            "    primes = []\n",
            "    for possiblePrime in range(2, n + 1):\n",
            "        # Assume number is prime until shown it is not. \n",
            "        isPrime = True\n",
            "        for num in range(2, int(possiblePrime ** 0.5) + 1):\n",
            "            if possiblePrime % num == 0:\n",
            "                isPrime = False\n",
            "                break\n",
            "        if isPrime:\n",
            "            primes.append(possiblePrime)\n",
            "    return primes\n",
            "\n",
            "# Example usage\n",
            "n = 50\n",
            "primes = generate_primes_trial_division(n)\n",
            "print(f\"Prime numbers up to {n}: {primes}\")\n",
            "```\n",
            "\n",
            "### Method 2: Sieve of Eratosthenes\n",
            "\n",
            "This method uses a boolean array to mark off composite numbers, leaving only the prime numbers.\n",
            "\n",
            "```python\n",
            "def generate_primes_sieve_of_eratosthenes(n):\n",
            "    \"\"\"\n",
            "    Generate prime numbers up to n using the Sieve of Eratosthenes.\n",
            "\n",
            "    Args:\n",
            "        n (int): The upper limit.\n",
            "\n",
            "    Returns:\n",
            "        list: A list of prime numbers up to n.\n",
            "    \"\"\"\n",
            "    sieve = [True] * (n + 1)\n",
            "    sieve[0:2] = [False, False]  # 0 and 1 are not prime numbers\n",
            "    for currentPrime in range(2, int(n ** 0.5) + 1):\n",
            "        if sieve[currentPrime]:\n",
            "            for multiple in range(currentPrime * currentPrime, n + 1, currentPrime):\n",
            "                sieve[multiple] = False\n",
            "    return [num for num, is_prime in enumerate(sieve) if is_prime]\n",
            "\n",
            "# Example usage\n",
            "n = 50\n",
            "primes = generate_primes_sieve_of_eratosthenes(n)\n",
            "print(f\"Prime numbers up to {n}: {primes}\")\n",
            "```\n",
            "\n",
            "### Comparison of Methods\n",
            "\n",
            "The Sieve of Eratosthenes algorithm is generally more efficient than the trial division method, especially for larger values of `n`. However, the trial division method can be simpler to understand and implement.\n",
            "\n",
            "### Time Complexity\n",
            "\n",
            "* Trial Division: O(n*sqrt(n))\n",
            "* Sieve of Eratosthenes: O(n log log n)\n",
            "\n",
            "Note: The time complexity is an estimate and may vary depending on the specific implementation and hardware.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the gradio interface"
      ],
      "metadata": {
        "id": "Cq147HL8LjGW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "def llama_model_response(message, history):\n",
        "  client = Groq(api_key = userdata.get(\"GROQ_API_KEY\"))\n",
        "  # Store the message history\n",
        "  messages = []\n",
        "  for msg in history:\n",
        "    messages.append({\n",
        "        \"role\": msg[\"role\"],\n",
        "        \"content\": msg[\"content\"]\n",
        "    })\n",
        "\n",
        "  # Add the current message in list\n",
        "  messages.append(\n",
        "      {\n",
        "          \"role\":\"user\",\n",
        "          \"content\": message\n",
        "      }\n",
        "  )\n",
        "\n",
        "  try:\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages = messages,\n",
        "        model = \"llama-3.3-70b-versatile\",\n",
        "        temperature = 0.7,\n",
        "        max_tokens = 2048\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "  except Exception as e:\n",
        "    return f\"Error : {str(e)}\"\n",
        "\n",
        "# Chat interface\n",
        "interface = gr.ChatInterface(\n",
        "    fn = llama_model_response,\n",
        "    type = \"messages\",\n",
        "    title = \"Llama 3.3 70b Chatbot\",\n",
        "    description= \"Created by Utkarsh Gaikwad\",\n",
        "    examples = [\n",
        "        \"Explain Generative AI in detail\",\n",
        "        \"Write a python code to generate prime numbers use most efficient algorithm\",\n",
        "        \"Explain Qunatum Computing in simple terms\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "N0inhonjLg9X",
        "outputId": "810c4644-80c4-4a9c-fb02-0da7ff31b63e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://babfc1006d4a48450b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://babfc1006d4a48450b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://babfc1006d4a48450b.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom response python coding expert\n",
        "adding system message"
      ],
      "metadata": {
        "id": "3tBOi8w2Pres"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from groq import Groq\n",
        "from google.colab import userdata\n",
        "\n",
        "def llama_model_response(message, history):\n",
        "  client = Groq(api_key = userdata.get(\"GROQ_API_KEY\"))\n",
        "  # Store the message history\n",
        "  messages = []\n",
        "  for msg in history:\n",
        "    messages.append({\n",
        "        \"role\": msg[\"role\"],\n",
        "        \"content\": msg[\"content\"]\n",
        "    })\n",
        "\n",
        "  # Add the current message in list\n",
        "  messages.append(\n",
        "      {\n",
        "          \"role\":\"system\",\n",
        "          \"content\": (\n",
        "              \"You are a coding assistant and expert in python\"\n",
        "              \"If any other question is asked apart form coding\"\n",
        "              \"Say i dont know\"\n",
        "            )\n",
        "      }\n",
        "  )\n",
        "  messages.append(\n",
        "      {\n",
        "          \"role\":\"user\",\n",
        "          \"content\": message\n",
        "      }\n",
        "  )\n",
        "\n",
        "  try:\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages = messages,\n",
        "        model = \"llama-3.3-70b-versatile\",\n",
        "        temperature = 0.7,\n",
        "        max_tokens = 2048\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content\n",
        "\n",
        "  except Exception as e:\n",
        "    return f\"Error : {str(e)}\"\n",
        "\n",
        "# Chat interface\n",
        "interface = gr.ChatInterface(\n",
        "    fn = llama_model_response,\n",
        "    type = \"messages\",\n",
        "    title = \"Llama 3.3 70b Chatbot\",\n",
        "    description= \"Created by Utkarsh Gaikwad\",\n",
        "    examples = [\n",
        "        \"Explain Generative AI in detail\",\n",
        "        \"Write a python code to generate prime numbers use most efficient algorithm\",\n",
        "        \"Explain Qunatum Computing in simple terms\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "interface.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "ll5OX6ryNh-c",
        "outputId": "afd5bdbe-52d6-4bbc-f9ca-8b2ebe82b6a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://43aae52d4ca42bbf0c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://43aae52d4ca42bbf0c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://43aae52d4ca42bbf0c.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "58yHqbt7QL_G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}